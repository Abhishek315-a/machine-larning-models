{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfIKRxNHT4NAfFENhqF9vB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek315-a/machine-larning-models/blob/main/Support_Vector_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine (SVM)\n",
        "Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification and regression tasks. Its main goal is to find the optimal hyperplane (decision boundary) that best separates classes in the feature space, maximizing the margin between the classes.\n",
        "\n",
        "***\n",
        "\n",
        "## How SVM Works\n",
        "\n",
        "### Basic Concepts:\n",
        "\n",
        "- **Hyperplane:** A line (in 2D), plane (3D), or a general hyperplane (n-dimensional) that separates classes:  \n",
        "  $$\n",
        "  \\mathbf{w} \\cdot \\mathbf{x} + b = 0\n",
        "  $$\n",
        "  where $$\\mathbf{w}$$ is the weight vector orthogonal to the hyperplane and $$b$$ is the bias.\n",
        "\n",
        "- **Margin:** The distance between the hyperplane and the closest data points of each class. SVM aims to maximize this margin.\n",
        "\n",
        "- **Support Vectors:** The subset of data points closest to the hyperplane. They define the position of the hyperplane.\n",
        "\n",
        "***\n",
        "\n",
        "### Formulation\n",
        "\n",
        "For a binary classification problem with data $$\\{(\\mathbf{x}_i, y_i)\\}$$ where $$y_i = \\pm 1$$,\n",
        "\n",
        "SVM solves this optimization problem:\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
        "$$\n",
        "\n",
        "subject to constraints:\n",
        "\n",
        "$$\n",
        "y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
        "$$\n",
        "\n",
        "This ensures data points are classified correctly with margin at least 1.\n",
        "\n",
        "***\n",
        "\n",
        "### Soft Margin (Allowing Misclassifications)\n",
        "\n",
        "For non-linearly separable data, introduce slack variables $$\\xi_i \\geq 0$$ and a penalty parameter $$C$$ to balance margin maximization and errors:\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
        "$$\n",
        "\n",
        "***\n",
        "\n",
        "### Kernel Trick\n",
        "\n",
        "When data is not linearly separable in original space, map data to a higher-dimensional space using a kernel function $$K(\\mathbf{x}_i, \\mathbf{x}_j)$$ without explicitly computing features:\n",
        "\n",
        "Common kernels:\n",
        "\n",
        "- Linear: $$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^\\top \\mathbf{x}_j$$\n",
        "- Polynomial: $$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j + r)^d$$\n",
        "- Radial Basis Function (RBF): $$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)$$\n",
        "\n",
        "***\n",
        "\n",
        "### Decision Function\n",
        "\n",
        "For a new data point $$\\mathbf{x}$$, classify based on:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b \\right)\n",
        "$$\n",
        "\n",
        "where $$\\alpha_i$$ are Lagrange multipliers from the dual problem, zero for non-support vectors.\n",
        "\n",
        "***\n",
        "\n",
        "## Summary\n",
        "\n",
        "- **Goal:** Find a hyperplane maximizing margin separating classes.\n",
        "- **Support Vectors:** Critical points defining the hyperplane.\n",
        "- **Soft Margin:** Allows trade-off between margin size and training error.\n",
        "- **Kernel Trick:** Maps data into higher dimension to handle non-linear separability.\n",
        "\n",
        "***\n",
        "\n",
        "This mathematical framework and kernel approach make SVM a powerful, versatile method for both linear and non-linear classification tasks.[1][2][3][5][7]\n",
        "\n",
        "[1](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-algorithm/)\n",
        "[2](https://www.ibm.com/think/topics/support-vector-machine)\n",
        "[3](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
        "[4](https://www.techtarget.com/whatis/definition/support-vector-machine-SVM)\n",
        "[5](https://www.geeksforgeeks.org/machine-learning/support-vector-machine-in-machine-learning/)\n",
        "[6](https://www.stratascratch.com/blog/machine-learning-algorithms-explained-support-vector-machine/)\n",
        "[7](https://scikit-learn.org/stable/modules/svm.html)\n",
        "[8](https://www.sciencedirect.com/topics/engineering/support-vector-machine)"
      ],
      "metadata": {
        "id": "DM9wYzvEkaVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Regression (SVR)\n",
        "Support Vector Regression (SVR) is an extension of Support Vector Machines (SVM) designed for regression tasks, aiming to predict continuous output values rather than categories. It works by finding a function that predicts the data points within a margin of tolerance while maintaining model simplicity.\n",
        "\n",
        "***\n",
        "\n",
        "## How SVR Works\n",
        "\n",
        "SVR attempts to fit a function $$f(\\mathbf{x})$$ that has at most $$\\varepsilon$$ deviation from the actual targets $$y_i$$ for all training data, and at the same time is as flat (simple) as possible.\n",
        "\n",
        "***\n",
        "\n",
        "### Basic objective\n",
        "\n",
        "Given training data $$\\{(\\mathbf{x}_i, y_i)\\}^n_{i=1}$$, SVR solves this optimization problem:\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w}, b, \\xi_i, \\xi_i^*} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^*)\n",
        "$$\n",
        "\n",
        "subject to constraints:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "y_i - (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\leq \\varepsilon + \\xi_i \\\\\n",
        "(\\mathbf{w} \\cdot \\mathbf{x}_i + b) - y_i \\leq \\varepsilon + \\xi_i^* \\\\\n",
        "\\xi_i, \\xi_i^* \\geq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "- $$\\mathbf{w}$$ is the weight vector (parameters).\n",
        "- $$b$$ is the bias term.\n",
        "- $$\\varepsilon$$ is the margin of tolerance where no penalty is given.\n",
        "- $$\\xi_i, \\xi_i^*$$ are slack variables allowing errors greater than $$\\varepsilon$$.\n",
        "- $$C$$ controls the trade-off between model flatness and tolerance to deviations larger than $$\\varepsilon$$.\n",
        "\n",
        "***\n",
        "\n",
        "### Explanation of terms:\n",
        "\n",
        "- **Flatness:** Minimize $$\\|\\mathbf{w}\\|^2$$ to keep the function $$f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b$$ as smooth as possible.\n",
        "- **Epsilon-insensitive tube:** Points within $$\\varepsilon$$ range of the function are considered correctly predicted without error.\n",
        "- **Slack variables:** Allow some points to lie outside the $$\\varepsilon$$-tube if exact fitting is impossible.\n",
        "- **C parameter:** Higher values of $$C$$ assign higher penalty for errors, leading to less tolerance and possibly overfitting.\n",
        "\n",
        "***\n",
        "\n",
        "### Kernel trick for non-linear SVR\n",
        "\n",
        "For non-linear relationships, SVR uses kernels $$K(\\mathbf{x}_i, \\mathbf{x}_j)$$ to map data into higher dimensional spaces implicitly, enabling linear regression in transformed space:\n",
        "\n",
        "The regression function becomes:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) K(\\mathbf{x}_i, \\mathbf{x}) + b\n",
        "$$\n",
        "\n",
        "where $$\\alpha_i, \\alpha_i^*$$ are learned coefficients from the dual problem formulation.\n",
        "\n",
        "Common kernels include linear, polynomial, and radial basis function (RBF).\n",
        "\n",
        "***\n",
        "\n",
        "### Summary\n",
        "\n",
        "- SVR fits a regression function within an epsilon margin, allowing some deviations penalized by slack variables.\n",
        "- It balances flatness (model simplicity) and prediction accuracy via hyperparameter $$C$$.\n",
        "- Uses kernels to handle non-linear regression tasks.\n",
        "- Robust to noise and outliers compared to classic regression models.\n",
        "\n",
        "***\n",
        "\n",
        "This framework makes SVR powerful and flexible for regression problems with linear or non-linear trends, especially when controlling model complexity is crucial.[1][2][3][4]\n",
        "\n",
        "[1](https://www.geeksforgeeks.org/machine-learning/support-vector-regression-svr-using-linear-and-non-linear-kernels-in-scikit-learn/)\n",
        "[2](https://www.sciencedirect.com/topics/computer-science/support-vector-regression)\n",
        "[3](https://www.scaler.com/topics/support-vector-regression/)\n",
        "[4](https://www.mathworks.com/help/stats/understanding-support-vector-machine-regression.html)\n",
        "[5](https://learninglabb.com/support-vector-regression-in-machine-learning/)"
      ],
      "metadata": {
        "id": "rt_ki3l0k0Y9"
      }
    }
  ]
}