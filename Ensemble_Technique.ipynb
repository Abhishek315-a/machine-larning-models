{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3OwZgN3Tb+AWpTHuIYyC9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek315-a/machine-larning-models/blob/main/Ensemble_Technique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques combine multiple machine learning models (\"learners\") to produce a more robust, accurate predictive result than any single model alone. Below is an explanation of ensemble methods, the main types, specific algorithms, formulas, and their variants.[1][2][3]\n",
        "\n",
        "***\n",
        "\n",
        "## Ensemble Technique\n",
        "\n",
        "- **Definition:** Ensemble learning merges predictions from several models to improve accuracy and robustness, leveraging the concept that a group’s consensus outperforms individual members.[2]\n",
        "- **Categories:** The main ensemble types are **bagging**, **boosting**, and **stacking**.\n",
        "\n",
        "***\n",
        "\n",
        "## Bagging (Bootstrap Aggregating)\n",
        "\n",
        "**How it works:**\n",
        "- Multiple models are trained in parallel using different random samples of the data (sampling with replacement, called bootstrapping).[4][5][1]\n",
        "- Final predictions are aggregated by majority voting (classification) or averaging (regression).[3][6][1]\n",
        "\n",
        "**Formula:**\n",
        "- For regression:  \n",
        "  $$\n",
        "  \\hat{y} = \\frac{1}{N}\\sum_{i=1}^{N}\\hat{y}_i\n",
        "  $$\n",
        "  where $$\\hat{y}_i$$ is the prediction of the $$i$$-th model.\n",
        "\n",
        "- For classification: The mode (most common) prediction among all models.\n",
        "\n",
        "**Variants:**\n",
        "- **Random Forest:** Bagging applied to decision trees, with additional random feature selection for each split.[7][8][9]\n",
        "- **Pasting:** Bagging without replacement (sampling is done without replacement).\n",
        "- **Random Subspaces:** Bagging on features instead of samples.\n",
        "\n",
        "***\n",
        "\n",
        "## Boosting\n",
        "\n",
        "**How it works:**\n",
        "- Models are trained sequentially. Each new model corrects errors made by its predecessor.[10][6][4]\n",
        "- Data points misclassified by earlier learners are weighted more heavily for subsequent learners.\n",
        "- Final predictions are a weighted sum or vote across models.\n",
        "\n",
        "**Formula (AdaBoost weights):**\n",
        "- Example weight update (binary classification):\n",
        "  $$\n",
        "  w_{i}^{(t+1)} = w_{i}^{(t)} \\cdot e^{\\alpha_t \\cdot I(y_i \\neq \\hat{y}_i)}\n",
        "  $$\n",
        "  where $$w_i^{(t)}$$ is weight of sample $$i$$ at round $$t$$, $$\\alpha_t$$ is the model's weight, $$I(\\cdot)$$ is an indicator function.[10]\n",
        "\n",
        "**Variants:**\n",
        "- **AdaBoost:** Uses weighted weak learners (often decision stumps).[7][10]\n",
        "- **Gradient Boosting:** Learners are sequentially trained to predict the residuals (errors) of previous models, minimizing a specified loss function.\n",
        "- **XGBoost:** An efficient, regularized implementation of gradient boosting.\n",
        "- **Stochastic Gradient Boosting:** Introduces random sub-sampling of the data at each stage.\n",
        "- **GBM, CatBoost, LightGBM:** Other gradient boosting frameworks with specific optimizations.\n",
        "\n",
        "***\n",
        "\n",
        "## Random Forest\n",
        "\n",
        "- **Type:** Bagging variant that trains many decision trees on random data and random subsets of features.[5][7]\n",
        "- **Formula:** Prediction is majority vote (classification) or average (regression) across all trees.\n",
        "- **Improves:** Reduces variance and avoids overfitting compared to individual trees.\n",
        "\n",
        "***\n",
        "\n",
        "## AdaBoost (Adaptive Boosting)\n",
        "\n",
        "- **Type:** Boosting variant focusing on correcting previous errors by reweighting.[7][10]\n",
        "- **Mechanism:** Trains weak classifiers sequentially, emphasizing misclassified samples and combining predictions using weights.\n",
        "- **Formula for model weight:**  \n",
        "  $$\n",
        "  \\alpha_t = \\frac{1}{2} \\ln \\left(\\frac{1 - e_t}{e_t}\\right)\n",
        "  $$\n",
        "  Where $$e_t$$ is error rate for iteration $$t$$.\n",
        "\n",
        "***\n",
        "\n",
        "## XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "- **Type:** Advanced gradient boosting framework optimizing speed and model complexity.[11]\n",
        "- **Formula:** Uses gradient descent and regularization (L1/L2) on additive trees.\n",
        "- **Advantages:** Handles overfitting and missing data efficiently. Regularizes via:\n",
        "  $$\n",
        "  Obj = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)\n",
        "  $$\n",
        "  Where $$l$$ is the loss, $$f_k$$ is the $$k^{th}$$ tree, and $$\\Omega$$ is the regularization term.\n",
        "\n",
        "***\n",
        "\n",
        "## Types of Bagging\n",
        "\n",
        "1. **Bootstrap aggregating (standard bagging):** Random sampling with replacement.\n",
        "2. **Random Forest:** Bagging over decision trees with random feature selection.\n",
        "3. **Pasting:** Bagging without replacement.\n",
        "4. **Random Subspaces:** Bagging on feature subsets.\n",
        "\n",
        "## Types of Boosting\n",
        "\n",
        "1. **AdaBoost:** Sequential weighted voting, usually decision stumps.\n",
        "2. **Gradient Boosting:** Sequential minimization of loss (GBM, XGBoost, LightGBM, CatBoost).\n",
        "3. **Stochastic Gradient Boosting:** Uses random subsampling at each boosting iteration.\n",
        "\n",
        "***\n",
        "\n",
        "| Technique      | Parallel/Sequential | Voting/Averaging | Example Algorithms         | Formula Summary                    |\n",
        "|----------------|--------------------|------------------|---------------------------|------------------------------------|\n",
        "| Bagging        | Parallel           | Majority/Average | Random Forest, Pasting    | $$ \\hat{y} = \\frac{1}{N}\\sum \\hat{y}_i $$ |\n",
        "| Boosting       | Sequential         | Weighted         | AdaBoost, XGBoost, GBM    | Weighted sum, or residual minimization |\n",
        "| Random Forest  | Parallel (Bagging) | Majority/Average | Random Forest             | As bagging with trees              |\n",
        "| AdaBoost       | Sequential (Boost) | Weighted         | AdaBoost                  | Weights updated by error rate      |\n",
        "| XGBoost        | Sequential (Boost) | Weighted & Reg.  | XGBoost                   | Gradient descent with regularization |\n",
        "\n",
        "***\n",
        "\n",
        "Ensemble techniques—by combining multiple model predictions—significantly improve accuracy, generalization, and robustness in machine learning tasks, with each method offering distinct strengths for various data scenarios.Ensemble methods combine several machine learning models to produce a more robust and accurate prediction than individual models. Below is a comprehensive explanation of ensemble techniques—including bagging, boosting, random forest, AdaBoost, and XGBoost—with formulas and explanations for their variants.[8][6][1][2][4][5][11]\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "GLl1AxfGUuUl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sb4rH3glVGra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}